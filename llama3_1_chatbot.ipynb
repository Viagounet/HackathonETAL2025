{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4bedc89-cbb6-40b8-92b0-51f85a9566f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidget.chatbot in /home/etal30/.local/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: ipywidgets>=8.1.7 in /home/etal30/.local/lib/python3.11/site-packages (from ipywidget.chatbot) (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipywidgets>=8.1.7->ipywidget.chatbot) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipywidgets>=8.1.7->ipywidget.chatbot) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipywidgets>=8.1.7->ipywidget.chatbot) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/etal30/.local/lib/python3.11/site-packages (from ipywidgets>=8.1.7->ipywidget.chatbot) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/etal30/.local/lib/python3.11/site-packages (from ipywidgets>=8.1.7->ipywidget.chatbot) (3.0.15)\n",
      "Requirement already satisfied: decorator in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/anaconda3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (0.2.5)\n",
      "Requirement already satisfied: executing in /usr/local/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /usr/local/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /usr/local/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (0.2.2)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.11/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets>=8.1.7->ipywidget.chatbot) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidget.chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc921a8-aed7-4384-91e1-8dec4602bea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in /home/etal30/.local/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/lib/python3.11/site-packages (2.32.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/etal30/.local/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/etal30/.local/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/anaconda3/lib/python3.11/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: decorator in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/anaconda3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /usr/local/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /usr/local/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /usr/local/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.11/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install ipywidgets requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4633e06b-c477-4058-aa0e-f622773cdc86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6856daf082f41a9ad9dfb9b0cb14a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid gray', border_left='1px solid gray', border_right='1px solid gra…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfd9e76d267421a94a231fe71f65b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(height='60px', width='100%'), placeholder='Type your message here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0200989b454b7bbac5c5e493cbea0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Send', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests, ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "BASE_URL = \"http://localhost:11434\"   # Ollama native API\n",
    "MODEL = \"llama3.1:latest\"             # your model\n",
    "\n",
    "# UI widgets510 \t1.143000\n",
    "520 \t1.080300\n",
    "530 \t1.062900\n",
    "540 \t1.119700\n",
    "550 \t1.029600\n",
    "560 \t1.053700\n",
    "570 \t1.049600\n",
    "580 \t1.041000\n",
    "590 \t1.183800\n",
    "600 \t1.049100\n",
    "610 \t1.084900\n",
    "620 \t1.062500\n",
    "630 \t1.082800\n",
    "640 \t1.125100\n",
    "650 \t1.016500\n",
    "660 \t1.079300\n",
    "670 \t1.060200\n",
    "680 \t1.022500\n",
    "690 \t1.148800\n",
    "700 \t1.152700\n",
    "710 \t1.059500\n",
    "720 \t1.082100\n",
    "730 \t1.010600\n",
    "740 \t0.993900\n",
    "750 \t1.095200\n",
    "760 \t1.037200\n",
    "770 \t1.044600\n",
    "780 \t1.097100\n",
    "790 \t1.067300\n",
    "800 \t0.997700\n",
    "810 \t1.105100\n",
    "820 \t0.985300\n",
    "830 \t1.085800\n",
    "840 \t1.110600\n",
    "850 \t0.983300\n",
    "860 \t1.020700\n",
    "870 \t0.949600\n",
    "880 \t1.131500\n",
    "890 \t0.965000\n",
    "900 \t1.048600\n",
    "910 \t1.078200\n",
    "920 \t1.035300\n",
    "930 \t1.097800\n",
    "940 \t1.136600\n",
    "950 \t1.064800\n",
    "960 \t0.994700\n",
    "970 \t0.965900\n",
    "980 \t1.069100\n",
    "990 \t1.062600\n",
    "1000 \t1.097000\n",
    "1010 \t1.082000\n",
    "1020 \t0.859700\n",
    "1030 \t0.928600\n",
    "1040 \t0.913200\n",
    "1050 \t0.862700\n",
    "1060 \t0.830500\n",
    "1070 \t0.884500\n",
    "1080 \t0.871900\n",
    "1090 \t0.936100\n",
    "1100 \t0.824400\n",
    "1110 \t0.928600\n",
    "1120 \t0.850000\n",
    "1130 \t0.887000\n",
    "1140 \t0.857600\n",
    "1150 \t0.837300\n",
    "1160 \t0.858600\n",
    "1170 \t0.871000\n",
    "1180 \t0.941700\n",
    "1190 \t0.882000\n",
    "1200 \t0.910300\n",
    "1210 \t0.786400\n",
    "1220 \t0.865000\n",
    "1230 \t0.924700\n",
    "1240 \t0.832300\n",
    "1250 \t0.880500\n",
    "1260 \t0.813600\n",
    "1270 \t0.847000\n",
    "1280 \t0.855400\n",
    "1290 \t0.895900\n",
    "1300 \t0.928200\n",
    "1310 \t0.839600\n",
    "1320 \t0.914500\n",
    "1330 \t0.796600\n",
    "1340 \t0.874000\n",
    "1350 \t0.887800\n",
    "1360 \t0.890800\n",
    "1370 \t0.907300\n",
    "1380 \t0.974700\n",
    "1390 \t0.866500\n",
    "1400 \t0.888200\n",
    "1410 \t0.916300\n",
    "1420 \t0.909300\n",
    "1430 \t0.908700\n",
    "1440 \t0.863200\n",
    "1450 \t0.886700\n",
    "1460 \t0.804600\n",
    "1470 \t0.868700\n",
    "1480 \t0.918700\n",
    "1490 \t0.919100\n",
    "1500 \t0.875700\n",
    "1510 \t0.867900\n",
    "chat_log = widgets.Output(layout={'border': '1px solid gray',\n",
    "                                  'height':'300px',\n",
    "                                  'overflow_y':'auto'})\n",
    "user_input = widgets.Textarea(\n",
    "    placeholder=\"Type your message here...\",\n",
    "    layout={'width':'100%','height':'60px'}\n",
    ")\n",
    "send_button = widgets.Button(description=\"Send\", button_style='primary')\n",
    "\n",
    "def send_message(_):\n",
    "    user_text = user_input.value.strip()\n",
    "    if not user_text:\n",
    "        return\n",
    "    user_input.value = \"\"  # clear box\n",
    "\n",
    "    # Show user text\n",
    "    with chat_log:\n",
    "        print(f\"You: {user_text}\")\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            f\"{BASE_URL}/api/chat\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": user_text}],\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=300\n",
    "        )\n",
    "        j = resp.json()\n",
    "        # Ollama /api/chat returns {\"message\":{\"role\":\"assistant\",\"content\":\"...\"}}\n",
    "        reply = j.get(\"message\", {}).get(\"content\") or j.get(\"response\") or \"(no reply)\"\n",
    "    except Exception as e:\n",
    "        reply = f\"Error: {e}\"\n",
    "\n",
    "    # Show bot reply\n",
    "    with chat_log:\n",
    "        print(f\"Bot: {reply}\\n\")\n",
    "\n",
    "send_button.on_click(send_message)\n",
    "\n",
    "display(chat_log, user_input, send_button)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b098009e-d285-4c1b-8236-34b2116a30f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Example use\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_step(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, can you help me check my report?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_step(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlso, remind me what I asked before.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mchat_step\u001b[0;34m(user_text)\u001b[0m\n\u001b[1;32m     11\u001b[0m history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_text})\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2) build input with chat template\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tok\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     15\u001b[0m     history,\n\u001b[1;32m     16\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,   \u001b[38;5;66;03m# so model knows it's its turn\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 3) generate\u001b[39;00m\n\u001b[1;32m     21\u001b[0m gen_cfg \u001b[38;5;241m=\u001b[39m GenerationConfig(\n\u001b[1;32m     22\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     23\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     25\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tok' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# Start with a system prompt (optional)\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "def chat_step(user_text):\n",
    "    global history\n",
    "    # 1) add the user message\n",
    "    history.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    # 2) build input with chat template\n",
    "    inputs = tok.apply_chat_template(\n",
    "        history,\n",
    "        add_generation_prompt=True,   # so model knows it's its turn\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 3) generate\n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids=inputs, generation_config=gen_cfg)\n",
    "\n",
    "    # 4) decode only the new assistant part\n",
    "    response = tok.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # 5) add assistant reply back into history\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example use\n",
    "print(\"Bot:\", chat_step(\"Hi, can you help me check my report?\"))\n",
    "print(\"Bot:\", chat_step(\"Also, remind me what I asked before.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b9f592-b007-4b13-98ee-1256ca9c7749",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "out-llama31-en-qlora is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/out-llama31-en-qlora/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:478\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     hf_hub_download(\n\u001b[1;32m    479\u001b[0m         path_or_repo_id,\n\u001b[1;32m    480\u001b[0m         filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    481\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    482\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    483\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    484\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    485\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    486\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    487\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    488\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    489\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    490\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1015\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1016\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1017\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1020\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1021\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[1;32m   1022\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1023\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1026\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1027\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1117\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1658\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1655\u001b[0m ):\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1546\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1546\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1547\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders, token\u001b[38;5;241m=\u001b[39mtoken, endpoint\u001b[38;5;241m=\u001b[39mendpoint\n\u001b[1;32m   1548\u001b[0m     )\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1463\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1464\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1465\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1466\u001b[0m     headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[1;32m   1467\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1468\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1469\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1470\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1471\u001b[0m )\n\u001b[1;32m   1472\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m    287\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    288\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    289\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[0;32m--> 310\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:459\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m     )\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-68b99891-64ba1b2503f335ac4aaba10c;fd74ba90-9f6c-4e58-8f2e-5de964c2de03)\n\nRepository Not Found for url: https://huggingface.co/out-llama31-en-qlora/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout-llama31-en-qlora\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1) Load tokenizer & model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tok \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_PATH)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     10\u001b[0m     MODEL_PATH,\n\u001b[1;32m     11\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1058\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m-> 1058\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m   1060\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:890\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    889\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 890\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    891\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    892\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    893\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    894\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    895\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    896\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    897\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    898\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    899\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    900\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    901\u001b[0m     _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    902\u001b[0m     _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    903\u001b[0m     _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    905\u001b[0m )\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:321\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    264\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    265\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    267\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    322\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:510\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[0;32m--> 510\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `hf auth login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    518\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: out-llama31-en-qlora is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# Path to your fine-tuned model folder\n",
    "MODEL_PATH = \"out-llama31-en-qlora\"\n",
    "\n",
    "# 1) Load tokenizer & model\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 2) Keep conversation history\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "def chat_step(user_text):\n",
    "    global history\n",
    "    # Add user input\n",
    "    history.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    # Build full input with conversation history\n",
    "    inputs = tok.apply_chat_template(\n",
    "        history,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids=inputs, generation_config=gen_cfg)\n",
    "\n",
    "    # Decode only the assistant's reply\n",
    "    response = tok.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Add assistant reply to history\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "print(\"Bot:\", chat_step(\"Hi, can you help me check my report?\"))\n",
    "print(\"Bot:\", chat_step(\"Also, remind me what I asked before.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58455f0a-f994-4d24-9b3a-90b81bc21c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac40c587b6f4ee58b1c75657e548585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid gray', border_left='1px solid gray', border_right='1px solid gra…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505831c2f8874e218467f06aa57e095e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(height='60px', width='100%'), placeholder='Type your message here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c02245b56e446199ae48b6e3c31c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Send', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests, ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "BASE_URL = \"http://localhost:11434\"   # Ollama’s default\n",
    "MODEL    = \"llama3.1:latest\"          # or \"llama3.1:8b-instruct\" if you have that tag\n",
    "\n",
    "chat_log = widgets.Output(layout={'border':'1px solid gray','height':'300px','overflow_y':'auto'})\n",
    "user_input = widgets.Textarea(placeholder=\"Type your message here...\", layout={'width':'100%','height':'60px'})\n",
    "send_button = widgets.Button(description=\"Send\", button_style='primary')\n",
    "\n",
    "def send_message(_):\n",
    "    txt = user_input.value.strip()\n",
    "    if not txt: return\n",
    "    user_input.value = \"\"\n",
    "    with chat_log: print(f\"You: {txt}\")\n",
    "\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            f\"{BASE_URL}/api/chat\",\n",
    "            json={\"model\": MODEL, \"messages\":[{\"role\":\"user\",\"content\":txt}], \"stream\": False},\n",
    "            timeout=120\n",
    "        )\n",
    "        j = r.json()\n",
    "        reply = j.get(\"message\",{}).get(\"content\") or j.get(\"response\") or \"(no reply)\"\n",
    "    except Exception as e:\n",
    "        reply = f\"Error: {e}\"\n",
    "\n",
    "    with chat_log: print(f\"Bot: {reply}\\n\")\n",
    "\n",
    "send_button.on_click(send_message)\n",
    "display(chat_log, user_input, send_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ed4b7f-bf40-4673-8506-6cd44417ad03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29afedcff1d845d6a9bcd54f11ec6a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid gray', border_left='1px solid gray', border_right='1px solid gra…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ddd234510d4a119a7b4825d110991f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(height='70px', width='100%'), placeholder='Type your message here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc646022392431d8a68a1492494cf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='primary', description='Send', style=ButtonStyle()), Button(button_style='w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json, os, requests, ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Configure Ollama ---\n",
    "BASE_URL = \"http://localhost:11434\"       # Ollama default\n",
    "MODEL    = \"llama3.1:latest\"              # or your exact tag, e.g. \"llama3.1:8b-instruct\"\n",
    "\n",
    "# --- Context controls ---\n",
    "MAX_TURNS = 30        # keep the last N user+assistant exchanges (plus system)\n",
    "SESSION_FILE = \"ollama_session.json\"  # persist across notebook restarts\n",
    "\n",
    "# --- Load / init history ---\n",
    "def load_history():\n",
    "    if os.path.exists(SESSION_FILE):\n",
    "        try:\n",
    "            return json.load(open(SESSION_FILE, \"r\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fresh start with a system message (tweak tone/persona as you like)\n",
    "    return [{\"role\":\"system\",\"content\":\"You are a concise, helpful assistant.\"}]\n",
    "\n",
    "def save_history(history):\n",
    "    try:\n",
    "        json.dump(history, open(SESSION_FILE, \"w\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def trim_history(history, max_turns=MAX_TURNS):\n",
    "    \"\"\"\n",
    "    Keep system + last N (user,assistant) turns.\n",
    "    \"\"\"\n",
    "    sys = [m for m in history if m[\"role\"]==\"system\"][:1]\n",
    "    rest = [m for m in history if m[\"role\"]!=\"system\"]\n",
    "    # each turn is 2 messages (user, assistant); keep last 2*MAX_TURNS\n",
    "    keep = rest[-2*max_turns:]\n",
    "    return sys + keep\n",
    "\n",
    "history = load_history()\n",
    "\n",
    "# --- UI ---\n",
    "chat_log   = widgets.Output(layout={'border':'1px solid gray','height':'300px','overflow_y':'auto'})\n",
    "user_input = widgets.Textarea(placeholder=\"Type your message here...\", layout={'width':'100%','height':'70px'})\n",
    "send_btn   = widgets.Button(description=\"Send\", button_style='primary')\n",
    "reset_btn  = widgets.Button(description=\"Reset memory\", button_style='warning')\n",
    "\n",
    "def send_message(_):\n",
    "    global history\n",
    "    user_text = user_input.value.strip()\n",
    "    if not user_text:\n",
    "        return\n",
    "    user_input.value = \"\"\n",
    "\n",
    "    # append user msg\n",
    "    history.append({\"role\":\"user\",\"content\":user_text})\n",
    "\n",
    "    # trim before sending (protect context window)\n",
    "    hist_to_send = trim_history(history)\n",
    "\n",
    "    with chat_log:\n",
    "        print(f\"You: {user_text}\")\n",
    "\n",
    "    try:\n",
    "        # Ollama chat API: https://github.com/ollama/ollama/blob/main/docs/api.md#chat\n",
    "        r = requests.post(\n",
    "            f\"{BASE_URL}/api/chat\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": hist_to_send,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=300\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        reply = j.get(\"message\",{}).get(\"content\") or j.get(\"response\") or \"(no reply)\"\n",
    "    except Exception as e:\n",
    "        reply = f\"Error: {e}\"\n",
    "\n",
    "    # append assistant msg & save/persist\n",
    "    history.append({\"role\":\"assistant\",\"content\":reply})\n",
    "    history = trim_history(history)\n",
    "    save_history(history)\n",
    "\n",
    "    with chat_log:\n",
    "        print(f\"FouFlou:{reply}\\n\")\n",
    "\n",
    "def reset_memory(_):\n",
    "    global history\n",
    "    history = [{\"role\":\"system\",\"content\":\"You are a concise, helpful assistant.\"}]\n",
    "    save_history(history)\n",
    "    with chat_log:\n",
    "        print(\"— memory cleared —\\n\")\n",
    "\n",
    "send_btn.on_click(send_message)\n",
    "reset_btn.on_click(reset_memory)\n",
    "display(chat_log, user_input, widgets.HBox([send_btn, reset_btn]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76326553-d03e-4aac-ac31-771788905e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
